# üß† Q-Learning Vacuum Cleaner Agent ‚Äî ChatGPT Debug Session

This document contains the full conversation between the user and ChatGPT while building, debugging, and improving a grid-based vacuum cleaning agent using Q-learning and Pygame.

---

## üì¶ Initial Code Overview

The user provided the following code:

```python
# See original conversation for full code snippet...
# The environment uses a 4x4 grid where each tile may or may not contain dirt.
# The agent learns to clean using tabular Q-learning.
```

---

## ‚ùì Issue: Agent Gets Stuck

> After training, the agent gets stuck in a loop between two rooms.

### üîç Root Cause

- The agent‚Äôs observation space is limited to:
  ```python
  (x, y, dirt)
  ```
- No knowledge of other dirty tiles
- Q-table only captures **local** info ‚Üí no guidance once tile is cleaned

---

## ‚úÖ Fixes Suggested

### Fix 1: Add visit penalties

Track visits to discourage loops:

```python
self.visits = np.zeros(self.grid_size, dtype=int)
```

In `step()`:

```python
self.visits[x, y] += 1
reward = -0.1 * self.visits[x, y]
if self.grid[x, y] == 1:
    self.grid[x, y] = 0
    reward += 10
```

---

### Fix 2: Render visit heatmap with red overlay

In `render()`:

```python
if env.visits[x, y] > 0:
    red_intensity = int(255 * (env.visits[x, y] / max_visits))
    red_surface = pygame.Surface((CELL_SIZE, CELL_SIZE), pygame.SRCALPHA)
    red_surface.fill((255, 0, 0, red_intensity // 2))
    screen.blit(red_surface, (y * CELL_SIZE, x * CELL_SIZE))
```

---

## üòñ Problem Still Happens

> The agent may loop without finishing the cleaning

---

## ‚ö°Ô∏è Quick Fix Attempt (Option 3)

1. Increase `epsilon` to 0.3 for more exploration  
2. Auto-reset environment after `done`  
3. Limit episode steps to prevent infinite loops  
4. Reward for visiting new tiles

```python
if self.visits[x, y] == 0:
    reward += 1
reward -= 0.1 * self.visits[x, y]
```

---

## ‚ùå Still Loops ‚Äî Root Problem: Partial State

Agent only knows `(x, y, dirt_here)` ‚Üí no clue about remaining dirt locations.

---

## ‚úÖ Solution: **Online Q-Learning**

Let agent continue learning during simulation.

### üîÅ `online_q_learning()`:

```python
def online_q_learning(env, Q_table, episodes=1000, alpha=0.1, gamma=0.9, epsilon=0.2):
    screen = pygame.display.set_mode(SCREEN_SIZE)
    clock = pygame.time.Clock()

    for ep in range(episodes):
        print(f"Episode {ep + 1}")
        state, _ = env.reset()
        done = False
        step_count = 0

        while not done and step_count < 100:
            for event in pygame.event.get():
                if event.type == pygame.QUIT:
                    pygame.quit()
                    return

            x, y, dirt = state
            if random.uniform(0, 1) < epsilon:
                action = env.action_space.sample()
            else:
                action = np.argmax(Q_table[x, y, dirt, :])

            next_state, reward, done, _, _ = env.step(action)
            nx, ny, ndirt = next_state

            Q_table[x, y, dirt, action] = (1 - alpha) * Q_table[x, y, dirt, action] + alpha * (
                reward + gamma * np.max(Q_table[nx, ny, ndirt, :])
            )

            state = next_state
            render(env, screen)
            clock.tick(5)

            step_count += 1

    pygame.quit()
```

---

## üíæ Save and Load Q-table

### Save Q-table

```python
def save_q_table(Q_table, filename="q_table.npy"):
    np.save(filename, Q_table)
    print(f"Q-table saved to {filename}")
```

### Load Q-table

```python
def load_q_table(filename="q_table.npy"):
    try:
        Q_table = np.load(filename)
        print(f"Q-table loaded from {filename}")
        return Q_table
    except FileNotFoundError:
        print("No saved Q-table found, starting fresh.")
        return None
```

### Usage Example

```python
Q_table = load_q_table()
if Q_table is None:
    Q_table = np.zeros((env.grid_size[0], env.grid_size[1], 2, env.action_space.n))

online_q_learning(env, Q_table)
save_q_table(Q_table)
```

---

## üìù Summary

| Feature                  | Status         |
|--------------------------|----------------|
| Visit penalty            | ‚úÖ Added        |
| Heatmap overlay          | ‚úÖ Added        |
| Exploration reward       | ‚úÖ Added        |
| Online learning          | ‚úÖ Working      |
| Q-table persistence      | ‚úÖ Save/Load added |
| Still loops sometimes?   | ‚ùå Root issue is partial state |

---

## üí° Next Steps (Optional)

- Use a DQN (Deep Q-Network) to learn from full grid state  
- Add state features like `remaining_dirt_count` or `distance_to_dirt`  
- Track total cleaned tiles per episode  
- Plot reward or steps per episode  

---

*Generated by ChatGPT ‚Äî April 2025*

